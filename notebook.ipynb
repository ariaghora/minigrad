{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import autograd as ag\n",
    "from autograd import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic gradient computation\n",
    "#### A computational graph perspective\n",
    "\n",
    "In autograd, we can represent a mathematical expression into a computational graph. Suppose we have:\n",
    "\\begin{align}\n",
    "c&=a+b\\\\\n",
    "d&=b+1\\\\\n",
    "e&=c*d\n",
    "\\end{align}\n",
    "\n",
    "Then the constructed graph is\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png\" width=300px/>\n",
    "\n",
    "Letting $a=2$ and $b=1$, we then have\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png\" width=300px/>\n",
    "\n",
    "Christopher Olah explained computational graph very well in [his blog](https://colah.github.io/posts/2015-08-Backprop/). Please refer to it for the details on the backpropagation step.\n",
    "\n",
    "Now, to see how awesome autograd in action is, let's define a simple function:\n",
    "\n",
    "$$y = a^2+3b$$\n",
    "\n",
    "We are going to find the gradient of $a$ and $b$. It means, we need to obtain the derivatives of $y$ with respect to $a$ and $b$. Suppose we fix $a=2$ and $b=1$. Now let's convert the problem into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 2\n",
    "a = Tensor(2, requires_grad=True)\n",
    "\n",
    "# b = 1\n",
    "b = Tensor(1, requires_grad=True)\n",
    "\n",
    "# y = a^2 + 3b\n",
    "y = ag.square(a) + (3 * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic lies in this following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients of all tensors that require grad.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the gradient of $a$?\n",
    "\\begin{align}\n",
    "\\frac{dy}{da}&={2a}^{2-1}+0 \\\\ &=2(2) \\\\ &=4\n",
    "\\end{align}\n",
    "\n",
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a:  4.0\n"
     ]
    }
   ],
   "source": [
    "print('Gradient of a: ', a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We got desired result. Now, the gradient of $b$ is supposed to be\n",
    "\\begin{align}\n",
    "\\frac{dy}{db}&=0+3(1) \\\\ &=3\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of b:  3.0\n"
     ]
    }
   ],
   "source": [
    "print('Gradient of b: ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we got desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example: neural network regressor\n",
    "\n",
    "Autograd is cool. But what makes it even cooler is that we can address a more practical real-world problem, in a flexible manner, like how [those](http://keras.io/), [major](http://pytorch.org/), deep learning [frameworks](http://deeplearning.net/software/theano/) do. Autograd is the heart of the optimization of many (deep) neural network models. \n",
    "\n",
    "As we have (somewhat) implemented an autograd system, let's try to make a neural network to regress house price. We are going to use boston housing dataset. For the sake of simplicity, let's just use sklearn to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Convert the feature and target into Tensors\n",
    "X = Tensor(X)\n",
    "y = Tensor(y.reshape(y.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape\n",
    "\n",
    "# num. of hidden neuron\n",
    "n_hidden = 50\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0001\n",
    "\n",
    "# input to hidden\n",
    "W1 = Tensor(np.random.randn(n_features, n_hidden), requires_grad=True)\n",
    "b1 = Tensor(np.zeros(n_hidden), requires_grad=True)\n",
    "\n",
    "# hidden to output\n",
    "W2 = Tensor(np.random.randn(n_hidden, 1), requires_grad=True)\n",
    "b2 = Tensor(np.zeros(1), requires_grad=True)\n",
    "\n",
    "def model_predict(X):\n",
    "    return (X @ W1 + b1) @ W2 + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "This following loop must be familiar: **gradient descent** algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0:  1103.4721258711595\n",
      "loss at iteration 10:  342.46672676395355\n",
      "loss at iteration 20:  154.8626813176444\n",
      "loss at iteration 30:  80.17453487186303\n",
      "loss at iteration 40:  49.23803982899968\n",
      "loss at iteration 50:  35.89179446234066\n",
      "loss at iteration 60:  29.715646094469804\n",
      "loss at iteration 70:  26.59498024537195\n",
      "loss at iteration 80:  24.879313581179908\n",
      "loss at iteration 90:  23.869976611801363\n",
      "loss at iteration 100:  23.245990912286192\n",
      "loss at iteration 110:  22.846039284678707\n",
      "loss at iteration 120:  22.582363283261426\n",
      "loss at iteration 130:  22.404264688788\n",
      "loss at iteration 140:  22.281201677964724\n",
      "loss at iteration 150:  22.194238388173098\n",
      "loss at iteration 160:  22.131389197763433\n",
      "loss at iteration 170:  22.084942460686168\n",
      "loss at iteration 180:  22.049864023183314\n",
      "loss at iteration 190:  22.02282094135569\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    # make a prediction\n",
    "    y_pred = model_predict(X)\n",
    "\n",
    "    # compute mean squared error loss, since we are dealing with regression\n",
    "    loss = ag.mean(ag.square(y_pred - y))\n",
    "    \n",
    "    # print the loss\n",
    "    if i % 10 == 0:\n",
    "        print('loss at iteration %d: ' % (i), loss.data)\n",
    "\n",
    "    # backpropagate the error\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    W1.data -= lr * W1.grad\n",
    "    W2.data -= lr * W2.grad\n",
    "    b1.data -= lr * b1.grad\n",
    "    b2.data -= lr * b2.grad\n",
    "\n",
    "    # zero the gradients of all nodes in the created graph\n",
    "    # so the gradients do not accumulate\n",
    "    loss.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the syntax and API calls are somewhat similar to those of pytorch. Basically we are doing the same thing. The difference is, pytorch has more optimization... And insanely smart team... And funding.\n",
    "\n",
    "Now let's try to plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y.data, label='true')\n",
    "plt.plot(y_pred.data, label='predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad fit, eh. You can do exercise to implement more operations so we can add more extension to this mini framework, e.g., nonlinear activation functions, performing classification, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
